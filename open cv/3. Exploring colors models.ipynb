{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93a9a8b",
   "metadata": {},
   "source": [
    "üé® Exploring Color Models in OpenCV\n",
    "\n",
    "In computer vision, different color models are used because each one highlights different image properties.\n",
    "\n",
    "1Ô∏è‚É£ Grayscale\n",
    "\t‚Ä¢\tRepresents image using only brightness (intensity).\n",
    "\t‚Ä¢\tReduces complexity and speeds up processing.\n",
    "\t‚Ä¢\tCommonly used for edge detection and feature extraction.\n",
    "\n",
    "2Ô∏è‚É£ RGB\n",
    "\t‚Ä¢\tRepresents colors using Red, Green, and Blue channels.\n",
    "\t‚Ä¢\tUsed mainly for image display and visualization.\n",
    "\t‚Ä¢\tNot ideal for color detection due to lighting sensitivity.\n",
    "\n",
    "3Ô∏è‚É£ BGR\n",
    "\t‚Ä¢\tSame as RGB but with Blue first instead of Red.\n",
    "\t‚Ä¢\tDefault color format used by OpenCV internally.\n",
    "\n",
    "4Ô∏è‚É£ HSV\n",
    "\t‚Ä¢\tSeparates color (Hue) from brightness (Value).\n",
    "\t‚Ä¢\tVery useful for color detection and segmentation.\n",
    "\t‚Ä¢\tWorks better under changing lighting conditions.\n",
    "\n",
    "5Ô∏è‚É£ LAB\n",
    "\t‚Ä¢\tDesigned to be close to human color perception.\n",
    "\t‚Ä¢\tSeparates lightness from color information.\n",
    "\t‚Ä¢\tUseful for color comparison and correction tasks.\n",
    "\n",
    "‚úÖ Summary:\n",
    "Each color model is chosen based on the task‚Äîspeed (Grayscale), display (RGB/BGR), color detection (HSV), or perceptual accuracy (LAB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e356777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv2.imread(\"pexels-pixabay-459203.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ca0bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 43, 127,  93],\n",
       "        [  2,  88,  54],\n",
       "        [  0,  79,  45],\n",
       "        ...,\n",
       "        [195,  95,   1],\n",
       "        [196,  96,   2],\n",
       "        [196,  96,   2]],\n",
       "\n",
       "       [[ 30, 114,  80],\n",
       "        [  2,  88,  54],\n",
       "        [  0,  86,  52],\n",
       "        ...,\n",
       "        [191,  91,   0],\n",
       "        [192,  92,   0],\n",
       "        [192,  92,   0]],\n",
       "\n",
       "       [[ 16, 101,  69],\n",
       "        [  0,  86,  53],\n",
       "        [  4,  91,  58],\n",
       "        ...,\n",
       "        [190,  90,   0],\n",
       "        [190,  90,   0],\n",
       "        [191,  91,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 83, 126, 159],\n",
       "        [ 90, 133, 166],\n",
       "        [100, 143, 176],\n",
       "        ...,\n",
       "        [ 30, 191, 123],\n",
       "        [ 22, 183, 115],\n",
       "        [ 18, 176, 109]],\n",
       "\n",
       "       [[122, 160, 195],\n",
       "        [117, 155, 190],\n",
       "        [111, 149, 184],\n",
       "        ...,\n",
       "        [ 23, 184, 114],\n",
       "        [ 15, 174, 104],\n",
       "        [  7, 164,  97]],\n",
       "\n",
       "       [[103, 141, 176],\n",
       "        [ 95, 133, 168],\n",
       "        [ 93, 131, 166],\n",
       "        ...,\n",
       "        [ 16, 177, 107],\n",
       "        [  6, 165,  95],\n",
       "        [  0, 152,  85]]], shape=(3443, 5531, 3), dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## converting bgr to grayscale\n",
    "## Grayscale: We use grayscale to remove color information and focus only on intensity, making processing faster and simpler for tasks like edge detection and shape analysis.\n",
    "gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(\"gray image.jpg\", gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934ed7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## converting bgr to HSV\n",
    "## HSV: We use HSV to separate color information (hue) from lighting (value), making color-based detection more robust to illumination changes.\n",
    "HSV_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2HSV)\n",
    "cv2.imwrite(\"hsv image.jpg\",HSV_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f2243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## converting bgr to lab\n",
    "## LAB: We use LAB because it is perceptually uniform and device-independent, so color differences match human vision and work well in color correction and comparison.\n",
    "lab_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2Lab)\n",
    "cv2.imwrite(\"lab image.jpg\",lab_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1e181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## converting bgr to rgb\n",
    "## RGB: We use RGB because it is the standard color model for image display and capture, directly matching how cameras and screens represent color.\n",
    "rgb_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite(\"rgb image.jpg\",lab_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4d337",
   "metadata": {},
   "source": [
    "#### BGR: We use BGR because some libraries like OpenCV store images in this order for historical and performance reasons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Global)",
   "language": "python",
   "name": "ml_global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
